{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constituency-Based Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 16 06:55:49 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "# set java path\n",
    "import os\n",
    "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_144\\bin\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "# scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "#                    path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "scp = StanfordParser(path_to_jar='C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser.jar',\n",
    "                   path_to_models_jar='C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser-3.7.0-models.jar')\n",
    "\n",
    "\n",
    "result = list(scp.raw_parse(sentence))\n",
    "print result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result[0].draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "print training_set[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VBZ -> 'cites',\n",
       " VBD -> 'spurned',\n",
       " PRN -> , ADVP-TMP ,,\n",
       " NNP -> 'ACCOUNT',\n",
       " JJ -> '36-day',\n",
       " NP-SBJ-2 -> NN,\n",
       " JJ -> 'unpublished',\n",
       " NP-SBJ-1 -> NNP,\n",
       " JJ -> 'elusive',\n",
       " NNS -> 'Lids']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "treebank_productions[0:10]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "\tt = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "\tfor production in t.productions():\n",
    "\t\ttreebank_productions.append(production)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0d44e19b3e31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get parse tree for sample sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviterbi_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Plamen\\AppData\\Local\\conda\\conda\\envs\\py2env1\\lib\\site-packages\\nltk\\parse\\viterbi.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# The most likely constituent table.  This table specifies the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Plamen\\AppData\\Local\\conda\\conda\\envs\\py2env1\\lib\\site-packages\\nltk\\grammar.pyc\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[1;32m--> 648\u001b[1;33m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: u\"'brown', 'fox', 'lazy', 'dog'\"."
     ]
    }
   ],
   "source": [
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we get an error when we try to parse our sample sentence tokens\n",
    "with our newly built parser. The reason is quite clear from the error: Some of the words\n",
    "in our sample sentence are not covered by the treebank -based grammar because they\n",
    "are not present in our treebank corpus. Now, because this constituency-based grammar\n",
    "uses POS tags and phrase tags to build the tree based on the training data, we will add the\n",
    "token and POS tags for our sample sentence in our grammar and rebuild the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get tokens and their POS tags\n",
    "from pattern.en import tag as pos_tagger\n",
    "tagged_sent = pos_tagger(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "\t\ttreebank_productions.append(production)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ-163 (DT The) (JJ brown) (NN fox))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (PRT (JJ quick))\n",
      "    (S\n",
      "      (CC and)\n",
      "      (NP-SBJ (PRP he))\n",
      "      (VP\n",
      "        (VBZ is)\n",
      "        (PP-1\n",
      "          (VBG jumping)\n",
      "          (NP (IN over) (DT the) (JJ lazy) (NN dog))))))) (p=2.02604e-48)\n"
     ]
    }
   ],
   "source": [
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)                                         \n",
    "\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "print result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result[0].draw()                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
