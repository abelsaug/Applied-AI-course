{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constituency-Based Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-parser\\.jar jar file at C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d58c0a585367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m scp = StanfordParser(path_to_jar='C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser.jar',\n\u001b[0;32m---> 21\u001b[0;31m                    path_to_models_jar='C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser-3.7.0-models.jar')\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser.jar"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 16 06:55:49 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "# set java path\n",
    "import os\n",
    "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_144\\bin\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "# scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "#                    path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "scp = StanfordParser(path_to_jar='C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser.jar',\n",
    "                   path_to_models_jar='C:/Software/StanfordNLP/stanford-parser-full-2016-10-31/stanford-parser-3.7.0-models.jar')\n",
    "\n",
    "\n",
    "result = list(scp.raw_parse(sentence))\n",
    "print result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result[0].draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "print training_set[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VBZ -> 'cites',\n",
       " VBD -> 'spurned',\n",
       " PRN -> , ADVP-TMP ,,\n",
       " NNP -> 'ACCOUNT',\n",
       " JJ -> '36-day',\n",
       " NP-SBJ-2 -> NN,\n",
       " JJ -> 'unpublished',\n",
       " NP-SBJ-1 -> NNP,\n",
       " JJ -> 'elusive',\n",
       " NNS -> 'Lids']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "treebank_productions[0:10]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "\tt = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "\tfor production in t.productions():\n",
    "\t\ttreebank_productions.append(production)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we get an error when we try to parse our sample sentence tokens\n",
    "with our newly built parser. The reason is quite clear from the error: Some of the words\n",
    "in our sample sentence are not covered by the treebank -based grammar because they\n",
    "are not present in our treebank corpus. Now, because this constituency-based grammar\n",
    "uses POS tags and phrase tags to build the tree based on the training data, we will add the\n",
    "token and POS tags for our sample sentence in our grammar and rebuild the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokens and their POS tags\n",
    "from pattern.en import tag as pos_tagger\n",
    "tagged_sent = pos_tagger(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "\t\ttreebank_productions.append(production)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ-163 (DT The) (JJ brown) (NN fox))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (PRT (JJ quick))\n",
      "    (S\n",
      "      (CC and)\n",
      "      (NP-SBJ (PRP he))\n",
      "      (VP\n",
      "        (VBZ is)\n",
      "        (PP-1\n",
      "          (VBG jumping)\n",
      "          (NP (IN over) (DT the) (JJ lazy) (NN dog))))))) (p=1.54897e-49)\n"
     ]
    }
   ],
   "source": [
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)                                         \n",
    "\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "print result[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].draw()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
