{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Necessary Depencies\n",
    "\n",
    "We will be leveraging several libraries and dependencies:\n",
    "• The nltk library, preferably version 3.1 or 3.2.1 or later\n",
    "• The spacy library\n",
    "• The pattern library\n",
    "• The Stanford parser\n",
    "• Graphviz and necessary libraries for the same\n",
    "\n",
    "\n",
    "For spacy , you need to first install\n",
    "the package and then separately install its dependencies, also called a language model .\n",
    "To install spacy , type pip install spacy from the terminal. Once done, download\n",
    "the English language model using the command python -m spacy.en.download from\n",
    "the terminal, which will download around 500 MB of data in the directory of the spacy\n",
    "package itself. For more details, refer to https://spacy.io/docs/#getting-started ,\n",
    "which tells you how to get started with using spacy . We will use spacy for tagging and\n",
    "depicting dependency-based parsing.\n",
    "\n",
    "$ pip install spacy\n",
    "$ python -m spacy.en.download\n",
    "\n",
    "\n",
    "To install pattern , typing pip install pattern should pretty much download and\n",
    "install the library and its necessary dependencies. The link www.clips.ua.ac.be/pages/\n",
    "pattern-en offers more information about pattern .\n",
    "\n",
    "$ pip install pattern\n",
    "\n",
    "The Stanford Parser is a Java-based implementation for a language parser developed\n",
    "at Stanford, which helps in parsing sentences to understand their underlying structure.\n",
    "We will perform both dependency and constituency grammar–based parsing using the\n",
    "Stanford Parser and nltk , which provides an excellent wrapper to leverage and use the\n",
    "parser from Python itself without the need to write code in Java. You can refer to the\n",
    "official installation guide at https://github.com/nltk/nltk/wiki/Installing-Third-\n",
    "Party-Software, which describes how to download and install the Stanford Parser and\n",
    "integrate it with nltk.\n",
    "To start with, make sure you first download and install the Java Development Kit\n",
    "(not just JRE, also known as Java Runtime Environment ) by going to www.oracle.com/\n",
    "technetwork/java/javase/downloads/index.html?ssSourceSiteId=otnjp . That is the\n",
    "official website. Java SE 8u101 / 8u102 are the latest versions at the time of writing this\n",
    "book—I have used 8u102 . After installing, make sure to set the “Path” for Java by adding it\n",
    "to the Path system environment variable. You can also create a JAVA_HOME environment\n",
    "variable pointing to the java.exe file belonging to the JDK. In my experience, neither\n",
    "worked for me when running the code from Python, and I had to explicitly use the\n",
    "Python os library to set the environment variable, which I will show when we dive into\n",
    "the implementation details. Once Java is installed, download the official Stanford Parser\n",
    "from http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip ,\n",
    "which seems to work quite well. You can try out a later version by going to http://nlp.\n",
    "stanford.edu/software/lex-parser.shtml#Download and checking the Release History\n",
    "section. After downloading, unzip it to a known location in your filesystem. Once done,\n",
    "you are now ready to use the parser from nltk.\n",
    "\n",
    "\n",
    "\n",
    "Graphviz is not really a necessity, and we will only be using it to view the dependency\n",
    "parse tree generated by the Stanford Parser. You can download Graphviz from its official\n",
    "website at www.graphviz.org/Download_windows.php and install it. Next, install pygraphviz ,\n",
    "which you can get by downloading the wheel file from www.lfd.uci.edu/~gohlke/\n",
    "pythonlibs/#pygraphviz, based on your system architecture and python version. Then\n",
    "install it using the command pip install pygraphviz-1.3.1-cp27-none-win_amd64.\n",
    "whl for a 64-bit system running Python 2.7.x . Once installed, pygraphviz should be ready\n",
    "to work.\n",
    "\n",
    "$ pip install pydot-ng\n",
    "$ pip install graphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', u'DET'), ('brown', u'ADJ'), ('fox', u'NOUN'), ('is', u'VERB'), ('quick', u'ADJ'), ('and', u'CONJ'), ('he', u'PRON'), ('is', u'VERB'), ('jumping', u'VERB'), ('over', u'ADP'), ('the', u'DET'), ('lazy', u'ADJ'), ('dog', u'NOUN')]\n",
      "[(u'The', u'DT'), (u'brown', u'JJ'), (u'fox', u'NN'), (u'is', u'VBZ'), (u'quick', u'JJ'), (u'and', u'CC'), (u'he', u'PRP'), (u'is', u'VBZ'), (u'jumping', u'VBG'), (u'over', u'IN'), (u'the', u'DT'), (u'lazy', u'JJ'), (u'dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 15 17:08:05 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "\n",
    "# recommended tagger based on PTB\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print tagged_sent\n",
    "\n",
    "from pattern.en import tag\n",
    "tagged_sent = tag(sentence)\n",
    "print tagged_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "# building your own tagger\n",
    "\n",
    "# preparing the data\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print train_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145415819537\n"
     ]
    }
   ],
   "source": [
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "\n",
    "print dt.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print dt.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240391131765\n"
     ]
    }
   ],
   "source": [
    "# regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "\n",
    "print rt.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print rt.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861361215994\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "print ut.evaluate(test_data)\n",
    "print ut.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.134669377481\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print bt.evaluate(test_data)\n",
    "print bt.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0806467228192\n",
      "[('The', u'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print tt.evaluate(test_data)\n",
    "print tt.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910155871817\n",
      "[('The', u'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', 'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, \n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "print ct.evaluate(test_data)        \n",
    "print ct.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930680607997\n",
      "[('The', u'DT'), ('brown', u'JJ'), ('fox', u'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'JJ'), ('dog', u'VBG')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "print nbt.evaluate(test_data)\n",
    "print nbt.tag(tokens)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.007\n",
      "             2          -0.76176        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/nltk/classify/maxent.py:1313: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "/usr/local/lib/python2.7/dist-packages/nltk/classify/maxent.py:1315: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/usr/local/lib/python2.7/dist-packages/nltk/classify/maxent.py:1316: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.984\n",
      "0.927001645851\n",
      "[('The', u'DT'), ('brown', u'NN'), ('fox', u'NN'), ('is', u'VBZ'), ('quick', u'JJ'), ('and', u'CC'), ('he', u'PRP'), ('is', u'VBZ'), ('jumping', u'VBG'), ('over', u'IN'), ('the', u'DT'), ('lazy', u'NN'), ('dog', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Do not run this in class - run out of memory\n",
    "# try this out for fun!\n",
    "met = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=MaxentClassifier.train)\n",
    "print met.evaluate(test_data)                           \n",
    "print met.tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
