{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Building Classifiers\n",
    "\n",
    "UIC CS 412, Spring 2018\n",
    "\n",
    "_If you have discussed this assignment with anyone, please state their name(s) here: [NAMES]. Keep in mind the expectations set in the Academic Honesty part of the syllabus._\n",
    "\n",
    "In this homework, you will build classifiers using decision trees, nearest neighbors, and perceptron, to make decisions on a few different datasets. The code for this project consists of several Python files, some of\n",
    "which you will need to read and understand in order to complete the\n",
    "assignment, and some of which you can ignore.\n",
    "\n",
    "This assignment is adapted from the github materials for [A Course in Machine Learning](https://github.com/hal3/ciml).\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at 11:59pm Tuesday, February 20th. \n",
    "\n",
    "### Files You'll Edit\n",
    "\n",
    "``dumbClassifiers.py``: This contains a handful of \"warm up\"\n",
    "classifiers to get you used to our classification framework.\n",
    "  \n",
    "``dt.py``: Will be your simple implementation of a decision tree classifier.\n",
    "  \n",
    "``knn.py``: This is where your nearest-neighbor classifier modifications\n",
    "will go.\n",
    "\n",
    "``perceptron.py``: The perceptron file you need to edit.\n",
    "\n",
    "### Files you might want to look at\n",
    "  \n",
    "``binary.py``: Our generic interface for binary classifiers (actually\n",
    "works for regression and other types of classification, too).\n",
    "\n",
    "``datasets.py``: Where a handful of test data sets are stored.\n",
    "\n",
    "``util.py``: A handful of useful utility functions: these will\n",
    "undoubtedly be helpful to you, so take a look!\n",
    "\n",
    "``runClassifier.py``: A few wrappers for doing useful things with\n",
    "classifiers, like training them, generating learning curves, etc.\n",
    "\n",
    "``mlGraphics.py``: A few useful plotting commands\n",
    "\n",
    "``data/*``: all of the datasets we'll use.\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "You will hand in all of the python files listed above together with your notebook **h2.ipynb** as a single zip file **h2.zip** on Gradescope.  Your notebook should contain the answers to the questions denoted by **WU#** below.\n",
    "\n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Your code will be autograded for technical correctness. Please **do\n",
    "not** change the names of any provided functions or classes within the\n",
    "code, or you will wreak havoc on the autograder. We have provided two simple test cases that you can try your code on, see ``run_tests_simple.py``. As usual, you should create more test cases to make sure your code runs correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Simple classifiers (5%)\n",
    "\n",
    "Let's begin our foray into classification by looking at some very\n",
    "simple classifiers.  There are two classifiers\n",
    "in ``dumbClassifiers.py``, one is implemented for you, the other\n",
    "one you will need to fill in appropriately.\n",
    "\n",
    "The already implemented one is ``AlwaysPredictOne``, a classifier that\n",
    "(as its name suggest) always predicts the positive class.  We're going\n",
    "to use the ``SentimentData`` dataset from ``datasets.py`` as a running\n",
    "example to test your functions.  Let's see how well \n",
    "this classifier does on this data.  You should begin by importing ``util``,\n",
    "``datasets``, ``binary`` and ``dumbClassifiers``.  Also, be sure you\n",
    "always have ``from numpy import *`` and ``from pylab import *``. You\n",
    "can achieve this with ``from imports import *`` to make life easier.\n",
    "\n",
    "We will look at a simple binary classification task: sentiment analysis (is this review a positive or negative evaluation of a product?). We'll use the presence/absence of words in the text as features. If you look in data/sentiment.all, you'll see the data for the sentiment prediction task. The first column contains the class value of zero or one (one = positive, zero = negative). The rest is a list of all the words that appear in this product reivew. These are binary features: any word listed has value \"=1\" and any word not listed has value \"=0\" (implicitly... it would be painful to list all non-occurring words!).\n",
    "As you write these functions, feel free to test your code on the much smaller TennisData dataset provided in datasets.py, so you can visually inspect correctness of your output. We have also provided some of the expected outputs as comments, so you can check whether you are getting the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dumbClassifiers, datasets, util\n",
    "from imports import *\n",
    "\n",
    "h = dumbClassifiers.AlwaysPredictOne({})\n",
    "print(h)\n",
    "# AlwaysPredictOne\n",
    "h.train(datasets.SentimentData.X, datasets.SentimentData.Y)\n",
    "h.predictAll(datasets.SentimentData.X)\n",
    "# array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it looks like it's always predicting one!\n",
    "\n",
    "Now, let's compare these predictions to the truth.  Here's a very\n",
    "clever way to compute accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean((datasets.SentimentData.Y > 0) == (h.predictAll(datasets.SentimentData.X) > 0))\n",
    "# 0.50416666666666665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's training accuracy; let's check test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean((datasets.SentimentData.Yte > 0) == (h.predictAll(datasets.SentimentData.Xte) > 0))\n",
    "# 0.50249999999999995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it does pretty badly.  That's not surprising, it's really not\n",
    "learning anything!!!\n",
    "\n",
    "Now, let's use some of the built-in functionality to help do some of\n",
    "the grunt work for us.  You'll need to import ``runClassifier``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runClassifier\n",
    "runClassifier.trainTestSet(h, datasets.SentimentData)\n",
    "# Training accuracy 0.5041666666666667, test accuracy 0.5025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very convenient!\n",
    "\n",
    "Now, your first implementation task will be to implement the missing\n",
    "functionality in ``AlwaysPredictMostFrequent`` in dumbClassifiers.py.  This actually\n",
    "will \"learn\" something simple.  Upon receiving training data, it will\n",
    "simply remember whether +1 is more common or -1 is more common.  It\n",
    "will then always predict this label for future data.  Once you've\n",
    "implemented this, you can test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = dumbClassifiers.AlwaysPredictMostFrequent({})\n",
    "runClassifier.trainTestSet(h, datasets.SentimentData)\n",
    "# Training accuracy 0.5041666666666667, test accuracy 0.5025\n",
    "print(h)\n",
    "# AlwaysPredictMostFrequent(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it does the same as ``AlwaysPredictOne``, but that's\n",
    "because +1 is more common in that training data (i.e., the majority class is '1'). \n",
    "\n",
    "We can use more runClassifier functions to generate learning curves and hyperparameter curves:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curve = runClassifier.learningCurveSet(dumbClassifiers.AlwaysPredictOne({}), datasets.SentimentData)\n",
    "runClassifier.plotCurve('AlwaysPredictOne on Sentiment Data', curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see how the accuracy changes as more training data is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Decision trees (45%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will build decision trees both using the python package sklearn and using your own function.\n",
    "\n",
    "## 2.1 Training (5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sentiment analysis dataset and transform the words in each review into a bag-of-words format (0 and 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import data\n",
    "\n",
    "X,Y,dictionary = data.loadTextDataBinary('data/sentiment.tr')\n",
    "print(X)\n",
    "print(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully loaded 1400 examples of sentiment training data. The vocabulary size is 3473 words; we can look at the first ten words (arbitrarily sorted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree of depth 1 on the sentiment analysis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(X, Y)\n",
    "# check the default values of the DecisionTreeClassifier parameters\n",
    "DecisionTreeClassifier?\n",
    "\n",
    "data.showTree(dt, dictionary)\n",
    "\n",
    "# bad?\n",
    "# -N-> class 1\t(333 for class 0, 533 for class 1)\n",
    "# -Y-> class 0\t(358 for class 0, 176 for class 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that if you only have one question you can ask about the review it's that you should ask if the review contains the word \"bad\" or not. If it does not (\"N\") then it's probably a positive review (by a vote of 533 to 333); if it does (\"Y\") then it's probable a negative review (by a vote of 358 to 176).\n",
    "\n",
    "Let's look at training accuracy for the tree of depth 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dt.predict(X) == Y)\n",
    "# 0.63642857142857145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not enough to just think about training data; we need to see how well these trees generalize to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xde,Yde,_ = data.loadTextDataBinary('data/sentiment.de', dictionary)\n",
    "np.mean(dt.predict(Xde) == Yde)\n",
    "# 0.60499999999999998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when we load the development data, we have to give it the dictionary we built on the training data so that words are mapped to integers in the same way!\n",
    "\n",
    "Here, we see that the accuracy has dropped a bit.\n",
    "\n",
    "Your first decision tree task is to build and show a decision tree of depth 2, and answer a few questions about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convince yourself whether or not it is useful to go from depth one to depth two on this data. How do you know?\n",
    "\n",
    "**WU1 (2%):** [YOUR ANSWER HERE]\n",
    "\n",
    "It's important to recognize that decision trees are essentially learning conjunctions of features. In particular, you can convert a decision tree to a sequence of if-then-else statements, of the form:\n",
    "\n",
    "if A and B and C and D then return POSITIVE elif A and B and C and !D then return NEGATIVE elif ...\n",
    "\n",
    "This is called a \"decision list.\" Write down the decision list corresponding to the tree that you learned of depth 2:\n",
    "\n",
    "**WU2 (1%):** [YOUR ANSWER HERE]\n",
    "\n",
    "Build a depth 3 decision tree and \"explain\" it. In other words, if your boss asked you to tell her, intuitively, what your tree is doing, how would you explain it? Write a few sentences.\n",
    "\n",
    "**WU3 (2%):** [YOUR ANSWER HERE]\n",
    "\n",
    "## 2.2 Underfitting and overfitting (10%)\n",
    "\n",
    "**WU4 (5%):** For all possible depths from depth 1 to depth 20, compute training error, development error and test error (on data/sentiment.te) for the corresponding decision tree (hint: use a for loop). Plot these three curves (yes, by hand if you must).\n",
    "\n",
    "[YOUR PLOT HERE]\n",
    "\n",
    "**WU5 (5%):** If you were to choose the depth hyperparameter based on TRAINING data, what TEST error would you get? If you were to choose depth based on the DEV data, what TEST error would you get? Finally, if you were to choose the depth based on the TEST data, what TEST error would you get. Precisely one of these three is \"correct\" -- which one and why?\n",
    "\n",
    "[YOUR ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementing a decision tree (30%)\n",
    "\n",
    "Our next task is to implement a decision tree classifier.  There is\n",
    "stub code in ``dt.py`` that you should edit.  Decision trees are\n",
    "stored as simple data structures.  Each node in the tree has\n",
    "a ``.isLeaf`` boolean that tells us if this node is a leaf (as\n",
    "opposed to an internal node).  Leaf nodes have a ``.label`` field\n",
    "that says what class to return at this leaf.  Internal nodes have:\n",
    "a ``.feature`` value that tells us what feature to split on;\n",
    "a ``.left`` *tree* that tells us what to do when the feature\n",
    "value is *less than 0.5*; and a ``.right`` *tree* that\n",
    "tells us what to do when the feature value is *at least 0.5*.\n",
    "To get a sense of how the data structure works, look at\n",
    "the ``displayTree`` function that prints out a tree.\n",
    "\n",
    "Your first task is to implement the training procedure for decision\n",
    "trees.  We've provided a fair amount of the code, which should help\n",
    "you guard against corner cases.  (Hint: take a look\n",
    "at ``util.py`` for some useful functions for implementing\n",
    "training.  Once you've implemented the training function, we can test\n",
    "it on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dt\n",
    "h = dt.DT({'maxDepth': 2})\n",
    "h.train(datasets.SentimentData.X, datasets.SentimentData.Y)\n",
    "h\n",
    "# this should print out something like this (the actual numbers attached to the branches will be different)\n",
    "#Branch 2428\n",
    "#  Branch 3842\n",
    "#    Leaf 1.0\n",
    "#    Leaf -1.0\n",
    "#  Branch 3892\n",
    "#    Leaf -1.0\n",
    "#    Leaf 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the branches here is that words have been converted into numeric ids\n",
    "for features. We can look them up. Your results here might be\n",
    "different due to hashing, so you will need to change them according to the branch numbers above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets.SentimentData.words[2428])\n",
    "#'bad'\n",
    "print(datasets.SentimentData.words[3842])\n",
    "#'worst'\n",
    "print(datasets.SentimentData.words[3892])\n",
    "#'sequence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we can rewrite the tree (by hand) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Branch 'bad'\n",
    "  Branch 'worst'\n",
    "    Leaf -1.0\n",
    "    Leaf 1.0\n",
    "  Branch 'sequence'\n",
    "    Leaf -1.0\n",
    "    Leaf 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should go implement prediction.  This should be easier than\n",
    "training!  We can test by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runClassifier.trainTestSet(dt.DT({'maxDepth': 1}), datasets.SentimentData)\n",
    "#Training accuracy 0.630833, test accuracy 0.595\n",
    "runClassifier.trainTestSet(dt.DT({'maxDepth': 3}), datasets.SentimentData)\n",
    "#Training accuracy 0.701667, test accuracy 0.6175\n",
    "runClassifier.trainTestSet(dt.DT({'maxDepth': 5}), datasets.SentimentData)\n",
    "#Training accuracy 0.765833, test accuracy 0.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it does better than the dumb classifiers on training data,\n",
    "as well as on test data!  Hopefully we can do even better in the\n",
    "future!\n",
    "\n",
    "We can use more ``runClassifier`` functions to generate learning\n",
    "curves and hyperparameter curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curve = runClassifier.learningCurveSet(dt.DT({'maxDepth': 9}), datasets.SentimentData)\n",
    "runClassifier.plotCurve('DT on Sentiment Data', curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots training and test accuracy as a function of the number of\n",
    "data points (x-axis) used for training and y-axis is accuracy.\n",
    "\n",
    "**WU6 (2%):** We should see training accuracy (roughly) going down and test\n",
    "accuracy (roughly) going up.  Why does training accuracy tend to go\n",
    "*down?* Why is test accuracy not monotonically increasing? You should\n",
    "also see jaggedness in the test curve toward the left. Why?\n",
    "\n",
    "[YOUR ANSWER HERE]\n",
    "\n",
    "We can also generate similar curves by changing the maximum depth\n",
    "hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = runClassifier.hyperparamCurveSet(dt.DT({}), 'maxDepth', [1,2,4,6,8,12,16], datasets.SentimentData)\n",
    "runClassifier.plotCurve('DT on Sentiment Data (hyperparameter)', curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the x-axis is the value of the maximum depth.\n",
    "\n",
    "**WU7 (3%):** You should see training accuracy monotonically increasing and\n",
    "test accuracy making something like a hill.  Which of these is\n",
    "*guaranteed* to happen and which is just something we might expect to\n",
    "happen?  Why?\n",
    "\n",
    "[YOUR ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Nearest Neighbors (30 %)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Warm-up exercise (0%)\n",
    "\n",
    "Our first task will be to use KNN to classify digits. In other words, we get an image \n",
    "of a hand-drawn digit (28x28 pixels, greyscale), and have to decide what digit it is. \n",
    "To make life simpler, we'll consider only the binary classification version, in two \n",
    "setups: (A) distinguishing ONEs from TWOs and (B) distinguishing TWOs from THREEs.\n",
    "\n",
    "(A) In the data directory, you'll find two .png files that show the training data. \n",
    "We are displaying them here. Are there any digits that you, as a human, have difficulty distinguishing \n",
    "(if so, list the row/column, where 0,0 is the upper left and 9,9 is the bottom right). \n",
    "Which of these (1vs2 or 2vs3) do you expect to be a harder classification problem?\n",
    "\n",
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"data/1vs2.tr.png\" width=\"60%\"></td>\n",
    "  <td><img src=\"data/2vs3.tr.png\" width=\"60%\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "\n",
    "(B) Let's verify that KNN does very well on training data. Run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import knn_warmup\n",
    "\n",
    "# importlib.reload(knn_warmup)\n",
    "\n",
    "tr = knn_warmup.loadDigitData(\"data/1vs2.tr\")\n",
    "te = knn_warmup.loadDigitData(\"data/1vs2.tr\", 100)\n",
    "allK = [1]\n",
    "print(\"\\t\".join([str(err) for err in knn_warmup.computeErrorRate(tr, te, allK)]))\n",
    "\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says \"do KNN, with 1vs2.tr as the training data and 1vs2.tr as the testing data, using K=1.\" \n",
    "The 0.0 is the error rate, which is zero. Verify the same thing for 2vs3.tr.\n",
    "\n",
    "(C) The ``knn_warmup.py`` implementation will let you specify multiple values for K and get error \n",
    "rates for all of them. In particular, you can say something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allK = [1,5,10,25,50,100]\n",
    "print(\"\\t\".join([str(err) for err in knn_warmup.computeErrorRate(tr, te, allK)]))\n",
    "\n",
    "# 0.0\t0.08\t0.12\t0.16\t0.28\t0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the same thing for six values of K (1, 5, ..., 100) and prints the respective \n",
    "error rates. Notice that for K=100 the error rate is 50% -- why does this happen?\n",
    "\n",
    "(D) Repeat the same exercise, this time evaluating on the development data, and using \n",
    "odd values of K ranging from 1 to 21. Do this for both 1vs2 and 2vs3. Which one is \n",
    "harder? For each, what is the optimal value of K? (In the case of ties, how would you \n",
    "choose to break ties?)\n",
    "\n",
    "(E) Now, go edit knn_warmup.py. This might take a bit of effort since you'll have to figure out \n",
    "what it's doing. But the function I want you to look at is \"classifyKNN.\" This takes D \n",
    "(the training data) and knn (the list of the K nearest neighbors, together with their \n",
    "distances). It iterates over each of the (dist,n) nearest neighbors. Here, dist is the \n",
    "distance and n is the training example id, so D[n] is the corresponding training example. \n",
    "It then \"votes\" this into a prediction ``yhat``.\n",
    "\n",
    "Modify this function so that each example gets a weighted vote, where its weight is \n",
    "equal to exp(-dist). This should be a one- or two-liner.\n",
    "\n",
    "Rerun the same experiments as in (D). Does this help or hurt? What do you observe as K \n",
    "gets larger and WHY do you observe this?\n",
    "\n",
    "If you want to play around, try exp(-dist / CONSTANT) where CONSTANT now is a hyperparameter. \n",
    "What happens as CONSTANT tends toward zero? Tends toward infinity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implementing a KNN classifier (20%)\n",
    "\n",
    "To get started with geometry-based classification, we will implement a\n",
    "nearest neighbor classifier that supports KNN classification.  \n",
    "This should go in ``knn.py``.  The\n",
    "only function here that you have to do anything about is\n",
    "the ``predict`` function, which does all the work.\n",
    "\n",
    "In order to test your implementation, here are some outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import knn\n",
    "\n",
    "runClassifier.trainTestSet(knn.KNN({'isKNN': True, 'K': 1}), datasets.TennisData)\n",
    "#Training accuracy 1, test accuracy 1\n",
    "runClassifier.trainTestSet(knn.KNN({'isKNN': True, 'K': 3}), datasets.TennisData)\n",
    "#Training accuracy 0.785714, test accuracy 0.833333\n",
    "runClassifier.trainTestSet(knn.KNN({'isKNN': True, 'K': 5}), datasets.TennisData)\n",
    "#Training accuracy 0.857143, test accuracy 0.833333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try it on the digits data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runClassifier.trainTestSet(knn.KNN({'isKNN': True, 'K': 1}), datasets.DigitData)\n",
    "# Training accuracy 1, test accuracy 0.94\n",
    "runClassifier.trainTestSet(knn.KNN({'isKNN': True, 'K': 3}), datasets.DigitData)\n",
    "# Training accuracy 0.94, test accuracy 0.93\n",
    "runClassifier.trainTestSet(knn.KNN({'isKNN': True, 'K': 5}), datasets.DigitData)\n",
    "# Training accuracy 0.92, test accuracy 0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WU8 (5%):** For the digits data, generate train/test curves for\n",
    "varying values of K (you figure out what are good ranges,\n",
    "this time).  Include those curves. Do you see evidence of overfitting and underfitting?  Next, using K=5, generate learning curves for this\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INCLUDE YOUR CODE FOR THE PLOTS HERE AND RUN THE CELL TO SHOW THE PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The curse of dimensionality (10%)\n",
    "\n",
    "If you have numpy and matplotlib correctly installed, you should be able to run\n",
    "the code in the following cell\n",
    "and get a picture of five histograms. Open up ``HighD.py`` to understand what's \n",
    "being plotted. Essentially, it's generating 200 random points in D dimensions \n",
    "(where D is being varied) and computing pairwise distances between these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HighD\n",
    "\n",
    "N    = 200                   # number of examples\n",
    "Dims = [2, 8, 32, 128, 512]   # dimensionalities to try\n",
    "Cols = ['#FF0000', '#880000', '#000000', '#000088', '#0000FF']\n",
    "Bins = arange(0, 1, 0.02)\n",
    "\n",
    "plt.xlabel('distance / sqrt(dimensionality)')\n",
    "plt.ylabel('# of pairs of points at that distance')\n",
    "plt.title('dimensionality versus uniform point distances')\n",
    "\n",
    "for i,d in enumerate(Dims):\n",
    "    distances = HighD.computeDistances(HighD.generateUniformDataset(d, N))\n",
    "    print(\"D={0}, average distance={1}\".format(d, mean(distances) * sqrt(d)))\n",
    "    plt.hist(distances,\n",
    "             Bins,\n",
    "             histtype='step',\n",
    "             color=Cols[i])\n",
    "    if HighD.waitForEnter:\n",
    "        plt.legend(['%d dims' % d for d in Dims])\n",
    "        plt.show(False)\n",
    "        x = raw_input('Press enter to continue...')\n",
    "\n",
    "\n",
    "plt.legend(['%d dims' % d for d in Dims])\n",
    "plt.savefig('fig.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the histograms, as the dimensionality increases, the distances between pairs of points become more or less concentrated around a single value.\n",
    "\n",
    "In the code, instead of plotting distance on the x-axis, we're plotting (distance/sqrt(D)). Why is this the right thing to do?\n",
    "\n",
    "The goal here is to look\n",
    "at whether what we found for uniformly random data points holds for\n",
    "naturally occurring data (like the digits data) too! We must hope that\n",
    "it doesn't, otherwise KNN has no hope of working, but let's verify.\n",
    "\n",
    "The problem is: the digits data is 784 dimensional, period, so it's\n",
    "not obvious how to try \"different dimensionalities.\" For now, we will\n",
    "do the simplest thing possible: if we want to have 128 dimensions, we\n",
    "will just select 128 features randomly.\n",
    "\n",
    "This is your task, which you can accomplish by munging together\n",
    "``HighD.py`` and ``KNN.py`` and making appropriate modifications.\n",
    "\n",
    "**WU9 (10%):** **A.** First, get a histogram of the raw digits data in 784\n",
    "dimensions. You'll probably want to use the ``exampleDistance``\n",
    "function from KNN together with the plotting in ``HighD``. \n",
    "\n",
    "**B.** Extend\n",
    "``exampleDistance`` so that it can subsample features down to some\n",
    "fixed dimensionality. For example, you might write\n",
    "``subsampleExampleDistance(x1,x2,D)``, where ``D`` is the target\n",
    "dimensionality. In this function, you should pick ``D`` dimensions at\n",
    "random (I would suggest generating a permutation of the number\n",
    "[1..784] and then taking the first D of them), and then compute the\n",
    "distance but _only_ looking at those dimensions. \n",
    "\n",
    "**C.** Generate an\n",
    "equivalent plot to HighD with D in [2, 8, 32, 128, 512] but for the\n",
    "digits data rather than the random data. Include a copy of both plots\n",
    "and describe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INCLUDE YOUR CODE FOR THE A,B,C PLOTS HERE AND RUN THE CELL TO SHOW THE PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Perceptron (20%)\n",
    "\n",
    "This final section is all about using perceptrons to make\n",
    "predictions. You are given a partial perceptron implementation in\n",
    "``perceptron.py``.\n",
    "\n",
    "The last implementation you have is for the perceptron; see\n",
    "``perceptron.py`` where you will have to implement part of the\n",
    "``nextExample`` function to make a perceptron-style update.\n",
    "\n",
    "Once you've implemented this, the magic in the ``Binary`` class will\n",
    "handle training on datasets for you, as long as you specify the number\n",
    "of epochs (passes over the training data) to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runClassifier.trainTestSet(perceptron.Perceptron({'numEpoch': 1}), datasets.TennisData)\n",
    "# Training accuracy 0.642857, test accuracy 0.666667\n",
    "runClassifier.trainTestSet(perceptron.Perceptron({'numEpoch': 2}), datasets.TennisData)\n",
    "# Training accuracy 0.857143, test accuracy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view its predictions on the two dimensional data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runClassifier.plotData(datasets.TwoDDiagonal.X, datasets.TwoDDiagonal.Y)\n",
    "h = perceptron.Perceptron({'numEpoch': 200})\n",
    "h.train(datasets.TwoDDiagonal.X, datasets.TwoDDiagonal.Y)\n",
    "print(h)\n",
    "# w=array([  7.3,  18.9]), b=0.0\n",
    "runClassifier.plotClassifier(array([ 7.3, 18.9]), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a linear separator that does a pretty good (but not\n",
    "perfect!) job classifying this data.\n",
    "\n",
    "Finally, we can try it on the sentiment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runClassifier.trainTestSet(perceptron.Perceptron({'numEpoch': 1}), datasets.SentimentData)\n",
    "# Training accuracy 0.835833, test accuracy 0.755\n",
    "runClassifier.trainTestSet(perceptron.Perceptron({'numEpoch': 2}), datasets.SentimentData)\n",
    "# Training accuracy 0.955, test accuracy 0.7975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WU10 (5%):** Using the tools provided, generate (a) a learning curve\n",
    "(x-axis=number of training examples) for the perceptron (5 epochs) on\n",
    "the sentiment data and (b) a plot of number of epochs versus\n",
    "train/test accuracy on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INCLUDE YOUR CODE FOR THE PLOTS HERE AND RUN THE CELL TO SHOW THE PLOTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {
   "height": "309px",
   "width": "468px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
